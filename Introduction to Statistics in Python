# Subset for Belgium and USA only
be_and_usa = food_consumption[(food_consumption['country'] == "Belgium") | (food_consumption['country'] == 'USA')]

# Group by country, select consumption column, and compute mean and median
print(be_and_usa.groupby('country')['consumption'].agg([np.mean, np.median]))

#####

# Print variance and sd of co2_emission for each food_category
print(food_consumption.groupby('food_category')['co2_emission'].agg([np.std, np.var]))

# Import matplotlib.pyplot with alias plt
import matplotlib.pyplot as plt

# Create histogram of co2_emission for food_category 'beef'
food_consumption[food_consumption['food_category'] == 'beef']['co2_emission'].hist()
# Show plot
plt.show()

#####

# Calculate total co2_emission per country: emissions_by_country
emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()

# Compute the first and third quantiles and IQR of emissions_by_country
q1 = np.quantile(emissions_by_country, 0.25)
q3 = np.quantile(emissions_by_country, 0.75)
iqr = q3 - q1

# Calculate the lower and upper cutoffs for outliers
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr

# Subset emissions_by_country to find outliers
outliers = emissions_by_country[(emissions_by_country < lower) | (emissions_by_country > upper)]
print(outliers)

#####

# Calculate probability of picking a deal with each product
probs = counts / amir_deals['product'].shape[0]

# Set random seed
np.random.seed(24)

# Sample 5 deals with replacement
sample_with_replacement = amir_deals.sample(5, replace = True)

##### Discrete distributions

# Create probability distribution
size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]
# Reset index and rename columns
size_dist = size_dist.reset_index()
size_dist.columns = ['group_size', 'prob']

# Expected value
expected_value = np.sum(size_dist['group_size'] * size_dist['prob'])

# Subset groups of size 4 or more
groups_4_or_more = size_dist['group_size'] >= 4

# Sum the probabilities of groups_4_or_more
prob_4_or_more = np.sum(groups_4_or_more * size_dist['prob'])
print(prob_4_or_more)

##### Generate random numbers according to uniform distribution

# Import uniform
from scipy.stats import uniform

# Generate 1000 wait times between 0 and 30 mins
wait_times = uniform.rvs(0, 30, size=1000)

# Create a histogram of simulated times and show plot
plt.hist(wait_times)
plt.show()

##### Binomial distribution

# Toss 1 coin once, and the probability of heads is 0.5
binom.rvs(1, 0.5, size = 1)

# Calculate the chance that toss 1 coin with a 0.5 probability of getting a heads and try to get 7 heads within 10 times
binom.pmf(7, 10, 0.5)

# Probability of closing <= 1 deal out of 3 deals
prob_less_than_or_equal_1 = binom.cdf(1, 3, 0.3)

# Probability of closing > 1 deal out of 3 deals
prob_greater_than_1 = 1 - binom.cdf(1, 3, 0.3)

##### Normal distribution

# Probability of deal < 7500 (mean = 5000, std = 2000)
prob_less_7500 = norm.cdf(7500, 5000, 2000)

# Calculate amount that 25% of deals will be less than (mean = 5000, std = 2000)
pct_25 = norm.ppf(0.25, 5000, 2000)

# Simulate 36 new sales
new_sales = norm.rvs(new_mean, new_sd, 36)

##### CLT

# Set seed to 104
np.random.seed(104)

sample_means = []
# Loop 100 times
for i in range(100):
  # Take sample of 20 num_users
  samp_20 = amir_deals['num_users'].sample(20, replace=True)
  # Calculate mean of samp_20
  samp_20_mean = np.mean(samp_20)
  # Append samp_20_mean to sample_means
  sample_means.append(samp_20_mean)
  
# Convert to Series and plot histogram (Calculate proportion)
sample_means_series = pd.Series(sample_means)
sample_means_series.hist()
# Show plot
plt.show()

##### Poisson distribution

# Import poisson from scipy.stats
from scipy.stats import poisson

# Probability of 2 or fewer responses (mean = 4)
prob_2_or_less = poisson.cdf(2, 4)

##### Correlation

# Create a scatterplot of happiness_score vs. life_exp and show
sns.scatterplot(x = "life_exp", y = "happiness_score", data = world_happiness)

# Create scatterplot of happiness_score vs life_exp with trendline
sns.lmplot(x='life_exp', y='happiness_score', data=world_happiness, ci=None)

# Correlation between life_exp and happiness_score
cor = world_happiness['life_exp'].corr(world_happiness['happiness_score'])

##### Log transformation (when data is highly skewed)

# Create log_gdp_per_cap column
world_happiness['log_gdp_per_cap'] = np.log(world_happiness['gdp_per_cap'])
